闭包:
闭包是由函数及其相关的引用环境组合而成的实体(即：闭包=函数+引用环境)
def addx(x):  
    def addy(y): 
        return x + y  
    return addy

func=addx(10)
func(10)
--->20


装饰器:
装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构
函数+实参高阶函数+返回值高阶函数+嵌套函数+语法糖 = 装饰器
def f1(func):
    def f2(*args, **kwargs):
        print('装饰器')
        func(*args, **kwargs)
    return f2

@f1
def f3(x):
    print('函数')
    print(x)

f3(x=3)


字符统计:
islower,isupper


数据库内外连接:
内连接：只展示相连两表匹配内容
左连接：左表展示所有内容，右表展示符合条件内容，其他则展示null
右链接：与左连接相反


三次握手
c  --SYN-->  s
s  --SYN+ACK-->  c
c  --ACK-->  s


深拷贝、浅拷贝
浅：只是对对象的引用
深：对对象的复制


排序算法
冒泡：
r=[5,3,2,7,4,9]
def swap(i, j):
    r[i],r[j]=r[j],r[i]

def bubble_sort_simple():
    lis =r
    length = len(r)
    for i in range(length):
        for j in range(i+1, length):
            if lis[i] > lis[j]:
                swap(i, j)

bubble_sort_simple()
print(r)

快速：
def quicksort(lists,left,right):
    if left >= right:
        return lists
    key = lists[left]
    low = left
    high = right
    while left < right:
        while left < right and lists[right] >= key:
            right -= 1
        lists[left] = lists[right]
        while left < right and lists[left] <= key:
            left += 1
        lists[right] = lists[left]
    lists[right] = key
    quicksort(lists,low,left-1)
    quicksort(lists,left+1,high)
    return lists
 
def main():
    lists = [11,23,5,2,44,88,42]
    length = len(lists)-1
    quicksort(lists,0,length)
    print (lists)
 


删除重复元素：
x=set(list)
x=list(x)


match（）和search（）区别：
math需要开头即匹配上，否则返货none
search则或匹配玩全部字符串


random模块
randint(s,e)-->[s,e]整数
random()-->[0,2)浮点
choice()-->随机挑选参数内容


python内存管理机制 ( Pymalloc ) 包括三个方面：引用计数、垃圾收集、内存池。下面分别予以阐述。
1.  引用计数：python程序中使用的每个变量后台都有一个引用计数。赋值或调用操作，计数加一；相反，删除或移出窗口对象，计数减一。
2.  垃圾收集：将引用计数为0的对象所占有的内存空间释放。还有一个循环垃圾收集器，负责清理未引用的循环，如两个对象互相引用的情况。
3.  内存池：内存池是预先从内存中申请的内存块，当创建小于256 bits 的对象时，从内存池申请内存空间。创建大于256 bits 的对象从内存申请空间。
    释放内存时，来自内存池的内存空间返回给内存池。这样做的目的是为了减少内存碎片，提升效率。


线程、进程、GIL


二分查找



二叉树



爬虫对网页解析的基本流程：
发起请求-->获取响应内容-->解析内容-->保存数据


scrapy工作流程：
engine通过调度器用request形式给downloader下载
下载完成后，engine发送给spider处理
处理得到item和新的request，将item发送给item pipeline，将request给调度器


一些反爬措施：
通过headers反爬虫：解决策略，伪造headers
基于用户行为反爬虫：动态变化去爬取数据，模拟普通用户的行为
通过动态更改代理ip来反爬虫
基于动态页面的反爬虫：跟踪服务器发送的ajax请求，模拟ajax请求,selnium和无界面浏览器


111


